{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XO2HTNRblvGM",
        "aDHhLVjuy2h4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5QJWnTDKknkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
        "%pip install -qq arxiv pymupdf"
      ],
      "metadata": {
        "id": "vKQ7XCTmknZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a199148-1d74-4189-821b-bccc695266e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HdtM1NA1khdL"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints._common import NVEModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-**********************************\"\n"
      ],
      "metadata": {
        "id": "hqmr6PI2vG1o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ],
      "metadata": {
        "id": "HVg7-_B_vK1H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO72IDY4vg-i",
        "outputId": "ccd6fcf8-3a96-4a45-89a0-6aa79e5ba6bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b']),\n",
              " Model(id='google/deplot', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot']),\n",
              " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct']),\n",
              " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b']),\n",
              " Model(id='adept/fuyu-8b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b']),\n",
              " Model(id='google/paligemma', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma']),\n",
              " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini']),\n",
              " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct']),\n",
              " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large']),\n",
              " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic']),\n",
              " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b']),\n",
              " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b']),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b']),\n",
              " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct']),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b']),\n",
              " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct']),\n",
              " Model(id='microsoft/kosmos-2', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2']),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b']),\n",
              " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct']),\n",
              " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2']),\n",
              " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b']),\n",
              " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b']),\n",
              " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct']),\n",
              " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct']),\n",
              " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct']),\n",
              " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b']),\n",
              " Model(id='nvidia/neva-22b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b']),\n",
              " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b']),\n",
              " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b']),\n",
              " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from functools import partial\n",
        "\n",
        "def RPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def PPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        pprint(preface, x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "NjHiYxkAvlVs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chatting with Large Documents**"
      ],
      "metadata": {
        "id": "TCXk9zlPvspZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JC1PHZYIlCOS",
        "outputId": "5b23fd5f-654a-43f4-8a8d-7c1ba5ce4628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA API Key: ··········\n",
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-F82...\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'ai-sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'ai-arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'ai-mixtral-8x22b': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'ai-phi-3-mini': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
              " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'ai-mixtral-8x22b-instruct': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
              " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
              " 'ai-mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'ai-arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
              " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
              " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
              " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
              " 'ai-llama3-8b': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'ai-google-paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
              " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'ai-dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'ai-phi-3-mini-4k': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
              " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
              " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'ai-seallm-7b': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
              " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
              " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'ai-llama3-70b': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
              " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'NV-Embed-QA': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'google/gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'aisingapore/sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'nv-rerank-qa-mistral-4b:1': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'snowflake/arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'mistralai/mixtral-8x22b-v0.1': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'microsoft/phi-3-mini-128k-instruct': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'microsoft/kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'meta/llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'mistralai/mixtral-8x22b-instruct-v0.1': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'mistralai/mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'snowflake/arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'google/deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'google/codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'mistralai/mixtral-8x7b-instruct-v0.1': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'google/gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'meta/llama3-8b-instruct': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'google/recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'google/paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'nvidia/neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'databricks/dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'microsoft/phi-3-mini-4k-instruct': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'adept/fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'mistralai/mistral-7b-instruct-v0.2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'seallms/seallm-7b-v2.5': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'meta/codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'meta/llama3-70b-instruct': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j7o9hwqZkmz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Naive Approach: Stuff Your Document"
      ],
      "metadata": {
        "id": "XO2HTNRblvGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a representation of the document and feeding it all to a chat model! From a document perspective, this is known as document stuffing.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fhxh9_cjl2Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=14DRI_uDviqzqg14TKoIc8IlBc3Zsb8oO\" width=800px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
        ">"
      ],
      "metadata": {
        "id": "TMM_ahuWmEzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a simple framework for connecting LLMs to your own data sources via general chunking strategies and strong incorporation with embedding frameworks/services. This framework has initially grown around its strong general support for LLM features, which signals its active strengths closer to the chain abstractions and agent coordination.\n",
        "\n",
        "LlamaIndex is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. It has since branched out to include general LLM capabilities similar to LangChain, but as of now it is still strongest in addressing the document side of LLM components since its initial abstractions were centered around that problem."
      ],
      "metadata": {
        "id": "cm-r8Du1mu2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Loading Documents\n"
      ],
      "metadata": {
        "id": "fJ5fZgeWm65S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a variety of document loaders to facilitate the injestion of various document formats (HTML, PDF, code) from many different sources and locations (local storage, private s3 buckets, public websites, messaging APIs, etc.). These loaders query your data sources and return a Document object which contains the content and metadata, usually in a plain-text or otherwise human-readible format. There are plenty of document loaders already built and ready to use, with the first-party LangChain options listed here."
      ],
      "metadata": {
        "id": "rvQ6Hjc7nDg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we can load a research paper of our choice using one of the following LangChain loaders:\n",
        "\n",
        "\n",
        "**UnstructuredFileLoader**: Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
        "\n",
        "\n",
        "**ArxivLoader**: A more specialized file-loader which can communicate with the Arxiv interface directly. Just one example of many, this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats).\n",
        "For our code example we will default to using ArxivLoader"
      ],
      "metadata": {
        "id": "WUfXVrHlnMIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpeBKfB0lIiu",
        "outputId": "852d49ea-1216-4afc-db83-0c52a6506e73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.63 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXSgDkf0n2MW",
        "outputId": "c9e3739a-ad48-4377-e9ec-570afb40c254"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain-community-0.2.1 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader, UnstructuredFileLoader"
      ],
      "metadata": {
        "id": "VSqffMCUnV6n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()"
      ],
      "metadata": {
        "id": "J6upLT0ZngIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = ArxivLoader(query=\"1706.03762\").load()  ## Attention is all you need"
      ],
      "metadata": {
        "id": "2rVP5shHn_e1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Documents Retrieved:\", len(documents))\n",
        "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
        "print(documents[0].page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISi_1GRwoV7p",
        "outputId": "c3095b60-91ef-4b0b-da24-d061c9f80464"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents Retrieved: 1\n",
            "Sample of Document 1 Content (Total Length: 39593):\n",
            "Provided proper attribution is provided, Google hereby grants permission to\n",
            "reproduce the tables and figures in this paper solely for use in journalistic or\n",
            "scholarly works.\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Exper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(documents[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ17HaBboj6J",
        "outputId": "5ad94ee6-166d-4230-ce29-4ed4fc569a40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion '\n",
            "            'Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
            " 'Published': '2023-08-02',\n",
            " 'Summary': 'The dominant sequence transduction models are based on complex '\n",
            "            'recurrent or\\n'\n",
            "            'convolutional neural networks in an encoder-decoder '\n",
            "            'configuration. The best\\n'\n",
            "            'performing models also connect the encoder and decoder through an '\n",
            "            'attention\\n'\n",
            "            'mechanism. We propose a new simple network architecture, the '\n",
            "            'Transformer, based\\n'\n",
            "            'solely on attention mechanisms, dispensing with recurrence and '\n",
            "            'convolutions\\n'\n",
            "            'entirely. Experiments on two machine translation tasks show these '\n",
            "            'models to be\\n'\n",
            "            'superior in quality while being more parallelizable and requiring '\n",
            "            'significantly\\n'\n",
            "            'less time to train. Our model achieves 28.4 BLEU on the WMT 2014\\n'\n",
            "            'English-to-German translation task, improving over the existing '\n",
            "            'best results,\\n'\n",
            "            'including ensembles by over 2 BLEU. On the WMT 2014 '\n",
            "            'English-to-French\\n'\n",
            "            'translation task, our model establishes a new single-model '\n",
            "            'state-of-the-art\\n'\n",
            "            'BLEU score of 41.8 after training for 3.5 days on eight GPUs, a '\n",
            "            'small fraction\\n'\n",
            "            'of the training costs of the best models from the literature. We '\n",
            "            'show that the\\n'\n",
            "            'Transformer generalizes well to other tasks by applying it '\n",
            "            'successfully to\\n'\n",
            "            'English constituency parsing both with large and limited training '\n",
            "            'data.',\n",
            " 'Title': 'Attention Is All You Need'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Though it may be tempting to accept the metadata format as-is and ignore the body entirely, there are a key selection of features that cannot be approached without diving into the full text:\n",
        "\n",
        "The metadata is not guaranteed. In the case of arxiv, paper abstracts, titles, authors, and date are necessary components of a submission, so being able to query them is not surprising. For an arbitrary PDF or webpage though, the same is not necessarily the case.\n",
        "The agent will not be able to go deeper into the document content. The summary is good to know and can be used as-is, but does not provide a straight-forward path to interacting with the body at any capacity (at least not from what we've learned).\n",
        "\n",
        "The agent will still not be able to reason about too many documents at once. Perhaps two summaries into one context and ask some questions. But what happens when you need to interact with 5 documents at once? What about an entire directory? Very soon, you will notice that your context window will be overloaded with information just to summarize or even list out the documents you're interested in!"
      ],
      "metadata": {
        "id": "l2y5ja9Uo1Lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Transforming The Documents"
      ],
      "metadata": {
        "id": "3fLiJc78pAs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once documents have been loaded, they often need to be transformed if we intend to pass them into our LLMs as context. One method of transformation is known as **chunking**, which breaks down large pieces of content into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
        "\n",
        "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [``RecursiveCharacterTextSplitter``](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us tp split our document with preference for some natural stopping points that we want our chunks to follow (as much as possible)."
      ],
      "metadata": {
        "id": "vCB7cQiYpLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "KlUbgU7Woca3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],)"
      ],
      "metadata": {
        "id": "wn8BlgNcpX7z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preprocessing"
      ],
      "metadata": {
        "id": "AdejDgjwpmKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
        "docs_split = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "9e8v6sdvpgmo"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def include_doc(doc):\n",
        "    ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
        "    string = doc.page_content\n",
        "    if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "30NxSR8ZprI3"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split = [doc for doc in docs_split if include_doc(doc)]"
      ],
      "metadata": {
        "id": "IoTXIG4yp0WP"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs_split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3benlsnp1-p",
        "outputId": "fc431d8c-01db-46e0-cef9-91c9a9cd2979"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in (0, 1, 2, 15, -1):\n",
        "    print(f'[{i}] ', docs_split[i].page_content)\n",
        "    print(\"-\"*64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdcA3HOp47_",
        "outputId": "9b91159b-c5d8-4afd-c63a-7fb4a2cd69b1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]  Provided proper attribution is provided, Google hereby grants permission to\n",
            "reproduce the tables and figures in this paper solely for use in journalistic or\n",
            "scholarly works.\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring significantly\n",
            "----------------------------------------------------------------\n",
            "[1]  be superior in quality while being more parallelizable and requiring significantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
            "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
            "best models from the literature. We show that the Transformer generalizes well to\n",
            "other tasks by applying it successfully to English constituency parsing both with\n",
            "large and limited training data.\n",
            "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
            "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
            "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "----------------------------------------------------------------\n",
            "[2]  detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
            "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
            "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
            "our research.\n",
            "†Work performed while at Google Brain.\n",
            "‡Work performed while at Google Research.\n",
            "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
            "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
            "1\n",
            "Introduction\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
            "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
            "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
            "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
            "architectures [38, 24, 15].\n",
            "----------------------------------------------------------------\n",
            "[15]  versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "4\n",
            "Why Self-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
            "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\n",
            "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
            "consider three desiderata.\n",
            "One is the total computational complexity per layer. Another is the amount of computation that can\n",
            "be parallelized, as measured by the minimum number of sequential operations required.\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range\n",
            "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
            "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
            "----------------------------------------------------------------\n",
            "[-1]  The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunking is pretty naive, but highlights the ease of getting at least something working for our application."
      ],
      "metadata": {
        "id": "Ov28UhAHqDVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Our approach for chunking is pretty naive, but highlights the ease of getting at least something working for our application. We made some effort to keep the chunk size small so that our models are able to wield it effectively as context, but how are we going to reason about all of these pieces?\n",
        "\n",
        "**When extending and optimizing this approach for an arbitrary set of documents, some potential options include:**\n",
        "\n",
        "- Identifying logical breaks or synthesis techniques (manually, automatically, LLM-assisted, etc).\n",
        "- Aiming to construct chunks that are rich in unique and relevant information, avoiding redundancy to maximize database utility.\n",
        "- Customizing chunking to fit the document’s nature, ensuring the chunks are contextually relevant and cohesive.\n",
        "- Including key concepts, keywords, or metadata snippets in each chunk for improved searchability and relevance in the database.\n",
        "- Continuously assessing chunking effectiveness and be ready to adjust strategies for optimal balance between size and content richness.\n",
        "- Considering a hierarchy system (implicitly-generated or explicitly-specified) to improve retrieval attempts.\n",
        "    - If interested, please look over the [**LlamaIndex tree structures from the index guide**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) as a starting point."
      ],
      "metadata": {
        "id": "xAg2i2ViqQeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Refining Summaries"
      ],
      "metadata": {
        "id": "vVA0J1s2qiX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a persuit to reason about large amounts of documents automatically, one potential idea might be to use LLMs to create a dense summary or knowledge base. Similar to how we maintained a running history of the conversation via slot-filling in the previous notebook, is there any problem with keeping a running history of an entire document?\n",
        "\n"
      ],
      "metadata": {
        "id": "L_c0Hfb1qq4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll be implementing a simple but useful Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. This process is commonly known as \"document refinement\" and is largely akin to our previous conversation-focused slot-filling exercise; the only difference is that now we're dealing with a large document instead of a growing chat history."
      ],
      "metadata": {
        "id": "EO-TVoR0rFlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>"
      ],
      "metadata": {
        "id": "fEAX_chSrGdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The DocumentSummary BaseModel"
      ],
      "metadata": {
        "id": "-Rp6bZMyrT9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Much like the `KnowledgeBase` class from the previous [notebook](https://github.com/kapardhi03/Rag-Agents/blob/main/Simple_RAG_Agents.ipynb), we can create a `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
      ],
      "metadata": {
        "id": "BJqTC2pJrZfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install IPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xhWS16UhtJlU",
        "outputId": "7b498fbb-1755-4642-860c-d50374efa2cd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
      ],
      "metadata": {
        "id": "5WZ4G7UZqCbd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "cKOsnk1as6Fo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentSummaryBase(BaseModel):\n",
        "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
        "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
        "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")"
      ],
      "metadata": {
        "id": "EPB0rLehtAEI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
        "        \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
        "        \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
        "        \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
        "    )), ('user', (\n",
        "        \"{format_instructions}. Follow the format precisely, including quotations and commas\\n\\n\"\n",
        "        \"{info_base}\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "rcxFl-grtibM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions': lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n"
      ],
      "metadata": {
        "id": "PpSCkjgytni0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_summary = \"\"\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    '''\n",
        "    Exercise: Create a chain that summarizes\n",
        "    '''\n",
        "    def summarize_docs(docs):\n",
        "        # Initialize the parse_chain appropriately; should include an RExtract instance.\n",
        "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
        "\n",
        "        state = {'info_base' : knowledge}\n",
        "\n",
        "        global latest_summary  ## If loop crashes, you can check out the latest_summary\n",
        "\n",
        "        for i, doc in enumerate(docs):\n",
        "            # Update the state as appropriate using your parse_chain component\n",
        "            state['input'] = doc.page_content\n",
        "            state = parse_chain.invoke(state)\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "    return RunnableLambda(summarize_docs)"
      ],
      "metadata": {
        "id": "g7k5UYyet5UZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = ChatNVIDIA(model=\"llama2_70b\") | StrOutputParser()\n",
        "instruct_llm = instruct_model | PydanticOutputParser(pydantic_object=DocumentSummaryBase)"
      ],
      "metadata": {
        "id": "MyyuzmbpyC7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf28e3c4-2194-4a39-c5fa-683f2abc5b4e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
        "try:\n",
        "  summary = summarizer.invoke(docs_split[:15])\n",
        "  # print(summary)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_uFN17AyENG",
        "outputId": "c3d48561-21e7-40f7-9902-d79ac36662a7",
        "collapsed": true
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid json output: running_summary: 'A new simple network architecture called the Transformer is proposed, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It shows superior quality and parallelization in machine translation tasks.'\n",
            "\n",
            "main_ideas:\n",
            "\n",
            "1. The Transformer network architecture is proposed, based on attention mechanisms only.\n",
            "2. It dispenses with recurrence and convolutions, making it more parallelizable.\n",
            "3. It shows superior quality in machine translation tasks.\n",
            "\n",
            "loose_ends:\n",
            "\n",
            "1. The attention mechanism used in the Transformer architecture is not fully explained.\n",
            "2. The paper does not provide a detailed comparison of the Transformer with other sequence transduction models.\n",
            "3. The authors do not discuss the potential limitations of the Transformer architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(latest_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRKLVDim4NZN",
        "outputId": "d8f99c0f-bde0-4295-bffc-56e5a7e2a013"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DocumentSummaryBase(running_summary='', main_ideas=[], loose_ends=[])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Synthetic Data Processing"
      ],
      "metadata": {
        "id": "aDHhLVjuy2h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Generality of Refinement**\n",
        "\n",
        "It's important to note that this \"progressive summarization\" technique is merely a starter chain that makes few assumptions about the initial data and desired output format. The same technique can be expanded far and wide to generate synthetic refinements with known metadata, active assumptions, and downstream objectives in mind.\n",
        "\n",
        "**Consider these potential applications:**\n",
        "\n",
        "1. **Aggregating Data**: Constructing structures that transform raw data from document chunks into coherent, useful summaries.\n",
        "2. **Categorization and Sub-topic Analysis**: Creating systems that categorize insights from chunks into defined categories, tracking emerging sub-topics within each.\n",
        "3. **Consolidation into Dense Informational Chunks**: Refining these structures to distill insights into compact segments, enriched with direct quotes for deeper analysis.\n",
        "\n",
        "These applications hint at the creation of a **domain-specific knowledge graph** which can be accessed and traversed by a conversational chat model. Some utilities already exist to generate these automatically via tools like [**LangChain Knowledge Graphs**](https://python.langchain.com/docs/modules/memory/types/kg). Though you might need to develop hierarchical structures and tools to both construct and traverse such a structure, it is a viable option when you can properly refine a sufficient knowledge graph for your use case! For this intereted in more advanced knowledge graph construction techniques which rely on larger systems and vector similarity, we found the [**LangChain x Neo4j Article**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) to be of interest.\n",
        "\n"
      ],
      "metadata": {
        "id": "ruBXhxyhzBDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbRwTwZ8yGNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}